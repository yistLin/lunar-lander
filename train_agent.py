# -*- coding: utf-8 -*-
"""Lunar Lander v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YA7QsZsgIil-8NiWezSeKHzt9lRRE5JT
"""

from argparse import ArgumentParser
from copy import deepcopy
from itertools import accumulate

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical

import gym


class Agent():
    """An agent trained with REINFORCE algorithm."""

    def __init__(self, device):
        self.network = nn.Sequential(
            nn.Linear(8, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 4),
            nn.Softmax(dim=-1),
        ).to(device)
        self.optimizer = optim.SGD(self.network.parameters(), lr=0.001)
        self.device = device

    def learn(self, log_probs, rewards, entropies):
        """Learn from recorded log probabilities and rewards."""
        log_probs = log_probs.to(self.device)
        rewards = rewards.to(self.device)
        entropies = entropies.to(self.device)

        loss = (-log_probs * rewards).sum() - 1e-4 * entropies.sum()

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def sample(self, state):
        """Sample an action from the network according to the given state."""
        state = torch.FloatTensor(state).to(self.device)

        action_prob = self.network(state)
        action_distribution = Categorical(action_prob)
        action = action_distribution.sample()
        log_prob = action_distribution.log_prob(action)
        entropy = action_distribution.entropy()

        return action.cpu().item(), log_prob, entropy


def train(env, agent, n_iters, batch_size, model_path):
    """Train an agent."""

    print("\n[Training]")
    print("========================")

    def reward_decay(a, b):
        return 0.99 * a + b

    agent.network.train()

    best_total_reward = float('-inf')

    avg_total_rewards, avg_final_rewards, avg_total_steps = [], [], []

    for batch in range(n_iters):

        log_probs, rewards, entropies = [], [], []
        total_rewards, final_rewards, total_steps = [], [], []

        for _ in range(batch_size):

            state = env.reset()
            total_reward, total_step = 0, 0

            episode_rewards = []

            while True:

                action, log_prob, entropy = agent.sample(state)
                log_probs.append(log_prob)
                entropies.append(entropy)

                state, reward, done, _ = env.step(action)

                total_reward += reward
                total_step += 1

                episode_rewards.append(reward)

                if done:
                    total_rewards.append(total_reward)
                    final_rewards.append(reward)
                    total_steps.append(total_step)

                    discounted_rewards = list(accumulate(
                        reversed(episode_rewards), reward_decay))[::-1]
                    rewards.append(np.array(discounted_rewards))

                    break

        avg_total_reward = sum(total_rewards) / len(total_rewards)
        avg_final_reward = sum(final_rewards) / len(final_rewards)
        avg_total_step = sum(total_steps) // len(total_steps)
        avg_total_rewards.append(avg_total_reward)
        avg_final_rewards.append(avg_final_reward)
        avg_total_steps.append(avg_total_step)
        print(f"Batch {batch+1:4d}, "
              f"total reward: {avg_total_reward: 6.1f}, "
              f"final reward: {avg_final_reward: 6.1f}, "
              f"total step: {avg_total_step:4d}")

        if (batch + 1) % 10 == 0:

            test_total_reward = test(env, agent, n_rounds=10)

            print(f"Test average total reward = {test_total_reward}")

            if test_total_reward > best_total_reward:
                print(f" => Improved from {best_total_reward} "
                      f"to {test_total_reward}")
                best_total_reward = test_total_reward
                best_state_dict = deepcopy(agent.network.state_dict())
                torch.save(best_state_dict, model_path)

            print()

        rewards = np.concatenate(rewards, axis=0)
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-9)
        agent.learn(torch.stack(log_probs),
                    torch.from_numpy(rewards),
                    torch.stack(entropies))

    return best_state_dict


def test(env, agent, n_rounds=10):
    """Test an agent."""

    print("\n[Testing]")
    print("========================")

    agent.network.eval()

    total_rewards = []

    for i in range(n_rounds):

        state = env.reset()
        total_reward, total_step = 0, 0

        while True:
            action, _, _ = agent.sample(state)
            state, reward, done, _ = env.step(action)
            total_reward += reward
            total_step += 1

            if done:
                print(f"episode {i+1:3d}, "
                      f"total_reward = {total_reward:6.1f}, "
                      f"total step: {total_step:4d}")
                total_rewards.append(total_reward)
                break

    agent.network.train()

    return sum(total_rewards) / len(total_rewards)


def main(model_path, n_iters, batch_size):
    """Train an agent to play Lunar Lander."""

    env = gym.make('LunarLander-v2')

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    agent = Agent(device)

    best_state_dict = train(env, agent, n_iters, batch_size, model_path)

    agent.network.load_state_dict(best_state_dict)
    test(env, agent)


def parse_args():
    """Parse command-line arguments."""
    parser = ArgumentParser()
    parser.add_argument("model_path", type=str, help="path to save model")
    parser.add_argument("-n", "--n_iters", type=int, default=1000)
    parser.add_argument("-m", "--batch_size", type=int, default=5)
    return vars(parser.parse_args())


if __name__ == "__main__":
    main(**parse_args())
