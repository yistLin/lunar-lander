# -*- coding: utf-8 -*-
"""Lunar Lander v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YA7QsZsgIil-8NiWezSeKHzt9lRRE5JT

# **Reinforcement Learning**
"""

from argparse import ArgumentParser
from copy import deepcopy
from itertools import accumulate

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical

import gym


class Agent():

    def __init__(self):
        self.network = nn.Sequential(
            nn.Linear(8, 64),
            nn.ReLU(),
            nn.Linear(64, 4),
            nn.Softmax(dim=-1)
        )
        self.optimizer = optim.Adam(self.network.parameters(), lr=0.001)

    def learn(self, log_probs, rewards, entropies):
        """Learn from recorded log probabilities and rewards."""
        loss = (-log_probs * rewards).sum() - 1e-3 * entropies.sum()
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def sample(self, state):
        """Sample an action from the network according to the given state."""
        action_prob = self.network(torch.FloatTensor(state))
        action_distribution = Categorical(action_prob)
        action = action_distribution.sample()
        log_prob = action_distribution.log_prob(action)
        entropy = action_distribution.entropy()
        return action.item(), log_prob, entropy


def train(env, agent, n_iters, batch_size):
    """Train an agent."""

    def reward_decay(a, b):
        return 0.99 * a + b

    agent.network.train()

    best_total_reward = float('-inf')

    avg_total_rewards, avg_final_rewards, avg_total_steps = [], [], []

    for batch in range(n_iters):

        log_probs, rewards, entropies = [], [], []
        total_rewards, final_rewards, total_steps = [], [], []

        for _ in range(batch_size):

            state = env.reset()
            total_reward, total_step = 0, 0

            episode_rewards = []

            while True:

                action, log_prob, entropy = agent.sample(state)
                log_probs.append(log_prob)
                entropies.append(entropy)

                state, reward, done, _ = env.step(action)

                total_reward += reward
                total_step += 1

                episode_rewards.append(reward)

                if done:
                    total_rewards.append(total_reward)
                    final_rewards.append(reward)
                    total_steps.append(total_step)

                    discounted_rewards = list(accumulate(
                        reversed(episode_rewards), reward_decay))[::-1]
                    rewards.append(np.array(discounted_rewards))

                    break

        avg_total_reward = sum(total_rewards) / len(total_rewards)
        avg_final_reward = sum(final_rewards) / len(final_rewards)
        avg_total_step = sum(total_steps) // len(total_steps)
        avg_total_rewards.append(avg_total_reward)
        avg_final_rewards.append(avg_final_reward)
        avg_total_steps.append(avg_total_step)
        print(f"Batch {batch+1:4d}, "
              f"total reward: {avg_total_reward: 6.1f}, "
              f"final reward: {avg_final_reward: 6.1f}, "
              f"total step: {avg_total_step:4d}")

        if avg_total_reward > best_total_reward:
            best_total_reward = avg_total_reward
            best_state_dict = deepcopy(agent.network.state_dict())
            print("best model so far")

        rewards = np.concatenate(rewards, axis=0)
        rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)
        agent.learn(torch.stack(log_probs), torch.from_numpy(
            rewards), torch.stack(entropies))

    return best_state_dict


def test(env, agent):
    """Test an agent."""

    agent.network.eval()

    for i in range(20):

        state = env.reset()
        total_reward = 0

        while True:
            action, _, _ = agent.sample(state)
            state, reward, done, _ = env.step(action)
            total_reward += reward
            if done:
                print(f"episode {i+1:3d}, total_reward = {total_reward:6.1f}")
                break


def main(model_path, n_iters, batch_size):
    """Train an agent to play Lunar Lander."""

    env = gym.make('LunarLander-v2')

    agent = Agent()

    print("\n[Training]")
    print("========================")

    best_state_dict = train(env, agent, n_iters, batch_size)
    torch.save(best_state_dict, model_path)

    print("\n[Testing]")
    print("========================")

    agent.network.load_state_dict(best_state_dict)
    test(env, agent)


def parse_args():
    """Parse command-line arguments."""
    parser = ArgumentParser()
    parser.add_argument("model_path", type=str, help="path to save model")
    parser.add_argument("-n", "--n_iters", type=int, default=1000)
    parser.add_argument("-m", "--batch_size", type=int, default=5)
    return vars(parser.parse_args())


if __name__ == "__main__":
    main(**parse_args())
